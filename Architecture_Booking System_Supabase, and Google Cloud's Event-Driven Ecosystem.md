# **Architecting a Modern Booking System: A Deep Dive into Next.js, Supabase, and Google Cloud's Event-Driven Ecosystem**

## **Part 1: The Foundational Layer: Project and Security Configuration**

The success of any modern, cloud-native application hinges on a meticulously configured and secured foundation. Before a single line of application logic is written, the underlying cloud infrastructure, data persistence layers, and security protocols must be established with production-grade rigor. A misconfiguration at this stage can introduce vulnerabilities and scalability bottlenecks that are difficult to re-engineer later. This section provides a comprehensive guide to initializing the Google Cloud and Supabase projects, configuring the necessary APIs, and implementing a multi-layered security strategy for managing sensitive credentials.

### **1.1 Google Cloud Project Initialization & API Configuration**

The Google Cloud Platform (GCP) will serve as the engine for all third-party integrations, from meeting creation to asynchronous event processing. Proper setup is paramount.

#### **Detailed Steps for Project and API Enablement**

A Google Cloud Project acts as the primary resource container for all APIs, credentials, and services.1 It can be created either through the web-based Google Cloud Console or programmatically via the

gcloud command-line interface (CLI).2

Once the project is created, several APIs must be enabled to support the booking system's functionality. This process associates the APIs with the project, enables monitoring, and activates billing for their usage.3 The following APIs are critical for this architecture:

1. **Google Calendar API:** This is the cornerstone of the meeting creation process. Instead of simply generating a raw video link, this API creates a full-fledged calendar event, which automatically handles Google Meet link generation, sends professional email invitations to attendees, and manages native calendar reminders.4
2. **Google Drive API:** This API is essential for the advanced, post-meeting features. Meeting recordings and transcripts generated by Google Meet are stored in the organizer's Google Drive. The Drive API will be used to programmatically access these artifacts, retrieve their metadata, and store shareable links back into the Supabase database.6
3. **Google Meet REST API:** While the Calendar API is the primary vehicle for _creating_ meetings, the Meet REST API is useful for managing existing meeting spaces and retrieving details about participants or conference artifacts after the fact. It should be enabled for architectural completeness and future extensibility.8
4. **Google Cloud Pub/Sub API:** This API is the linchpin of the event-driven architecture discussed in Part 3\. It provides the messaging bus that decouples the initial booking confirmation from the backend processing tasks.10

To enable these APIs, navigate to the "APIs & Services" \> "Library" section in the Google Cloud Console. Search for each API by name and click "Enable".1 It is crucial to understand that enabling an API is not merely a technical step; it signifies an agreement to the terms of service and, for many services, activates billing responsibility.1 Therefore, a production-ready setup must include proactive cost and quota management. Google APIs have per-minute and per-project usage quotas. Exceeding these can result in

403 usageLimits or 429 rateLimitExceeded errors, which can disrupt service.4 It is a best practice to configure billing alerts in the GCP console and implement robust error handling, such as exponential backoff, in the application code to gracefully manage these limits.

### **1.2 Mastering OAuth 2.0 for Offline, Server-Side Access**

For the backend system to act on behalf of a user (e.g., a consultant creating a meeting on their calendar) without their continuous interaction, it requires a specific type of authorization: the OAuth 2.0 web server flow with offline access. This flow allows the server to obtain a long-lived **refresh token**, which can be used to generate new, short-lived **access tokens** indefinitely.12

#### **Configuring the OAuth Consent Screen and Credentials**

The first step is to configure the OAuth Consent Screen. This is the user-facing prompt that explains which application is requesting access to their data and what permissions it needs.15 For a public-facing application, the user type should be set to "External." You must provide an app name, user support email, and authorized domains.15

Next, create OAuth 2.0 client credentials. For this server-side architecture, select "Web application" as the application type.14 This will generate a

**Client ID** and a **Client Secret**, which are essential for the application to identify itself to Google's authentication servers. These credentials should be downloaded as a client_secret.json file and stored securely; they must never be exposed on the client side.14

A critical part of this setup is defining the **Authorized redirect URIs**. This is the endpoint where Google will send the response after the user grants consent. For a server-side flow, this will be a backend endpoint in your application responsible for handling the OAuth callback.14

#### **The Importance of a Dedicated OAuth Flow for the Backend**

A common pitfall is attempting to use the provider_refresh_token that Supabase's built-in Google Auth provider returns after a user signs in.17 While convenient, this token is intended for immediate, client-side use and is not suitable for a long-term, server-side process. The refresh token is issued to Supabase's OAuth client, not the application's. Attempting to refresh this token from a different origin, such as a Supabase Edge Function or a Google Cloud Function, will likely result in an

invalid_grant error.19 This is because the request violates the OAuth 2.0 specification, which dictates that refresh tokens must only be used by the client to whom they were originally issued.20

Therefore, the correct architecture requires an independent OAuth 2.0 flow initiated by the application itself, specifically for the users who will be creating meetings (e.g., consultants). This flow must explicitly request the necessary scopes and offline access.

#### **The Offline Access Flow**

To obtain a refresh token, the authorization request to Google must include two critical parameters:

- access_type=offline: This parameter explicitly tells Google's authorization server to return a refresh token alongside the access token during the initial code exchange.12
- prompt=consent: While not always strictly required, including this parameter ensures the user is prompted for consent every time, which can force the re-issuance of a refresh token if one was not provided previously.21

The scopes requested must align with the APIs the application needs to access. For this system, the essential scopes are:

- https://www.googleapis.com/auth/calendar: To create and manage calendar events.
- https://www.googleapis.com/auth/drive.readonly: To access meeting recordings and transcripts later.

The flow proceeds as follows:

1. The consultant user initiates the connection from the application's UI.
2. The application redirects the user to Google's authorization URL with the client_id, redirect_uri, scope, and access_type=offline parameters.
3. The user authenticates and consents.
4. Google redirects the user back to the application's specified redirect_uri with an authorization code.
5. The application's server-side logic receives this code and makes a secure, backend POST request to Google's token endpoint (https://oauth2.googleapis.com/token).
6. This request includes the code, client_id, client_secret, and grant_type=authorization_code.
7. Google validates the request and returns a JSON object containing the access_token and, crucially, the refresh_token.

This refresh_token is the key to long-term, offline access. It must be stored securely, as it will be used by the backend to generate new access tokens whenever a booking needs to be processed.

### **1.3 Supabase Project and Database Schema Setup**

Supabase serves as the application's primary backend, providing the PostgreSQL database, user authentication, and serverless functions.23 Your project already contains a comprehensive database schema that handles detailed availability, services, and customer relationship management. For the purposes of this Google API integration, we will focus on the tables that are most relevant to the booking and meeting creation flow:

bookings, services, and profiles.

While your existing schema is robust, it lacks a dedicated, secure table for storing the Google OAuth refresh tokens obtained in the previous step. Storing these long-lived, sensitive credentials requires a specialized approach. Therefore, we will add one essential table to your schema: user_tokens.

#### **Project Creation and Local Setup**

First, create a new project in the Supabase Dashboard, providing a project name, a strong database password, and selecting a region close to the user base to minimize latency.24 Once the project is running, link it to a local development environment using the Supabase CLI:

Bash

\# Link to your remote Supabase project  
supabase link \--project-ref \<your-project-id\>

\# Pull the initial database schema  
supabase db pull

This establishes a local representation of the database, allowing for schema changes to be developed and tested locally before being applied to the remote database.23

#### **Database Schema Definition**

The following SQL statement defines the user_tokens table required for securely storing Google OAuth refresh tokens. This should be saved in a new migration file within the supabase/migrations directory to be version-controlled and applied to your database.

**User Tokens Table:** This table is dedicated to securely storing the Google OAuth refresh tokens for consultant users.

SQL

\-- supabase/migrations/YYYYMMDDHHMMSS_add_user_tokens_table.sql

CREATE TABLE public.user_tokens (  
 user_id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,  
 encrypted_google_refresh_token BYTEA NOT NULL,  
 created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),  
 updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()  
);

\-- Add comments for clarity  
COMMENT ON TABLE user_tokens IS 'Securely stores encrypted Google OAuth refresh tokens for users.';  
COMMENT ON COLUMN user_tokens.user_id IS 'The consultant user who owns the token.';  
COMMENT ON COLUMN user_tokens.encrypted_google_refresh_token IS 'The Google refresh token, encrypted at rest.';

This table will work in conjunction with your existing bookings and services tables to orchestrate the meeting creation process.

### **1.4 Fortifying the Backend: Secure Token Storage with Supabase Vault and RLS**

Storing sensitive credentials like OAuth refresh tokens requires the highest level of security. Storing them as plaintext in the database is unacceptable. Supabase provides a robust solution for this: **Supabase Vault**.

While Supabase previously offered encryption via the pgsodium extension, it is now pending deprecation due to its operational complexity and risk of misconfiguration. Supabase Vault is the recommended, modern replacement, providing a user-friendly layer on top of the same powerful cryptographic libraries.26

#### **Implementing Encryption with Supabase Vault**

Supabase Vault allows secrets to be stored encrypted at rest within the database. The encryption key itself is managed by Supabase's backend systems and is never exposed directly in SQL, providing a critical layer of separation.27

1. **Enable the Vault Extension:** Although often enabled by default, ensure the vault extension is active by navigating to the "Database" \> "Extensions" page in the Supabase Dashboard and enabling supabase_vault.27
2. **Storing the Refresh Token:** When the application receives a refresh token from the OAuth flow described in section 1.2, it should be immediately stored in the user_tokens table. Instead of a direct INSERT, the value will be encrypted using Vault's capabilities. The encrypted_google_refresh_token column was defined as BYTEA to hold the encrypted binary data. The encryption logic will be handled within a secure database function.

#### **The Secure Access Pattern: SECURITY DEFINER Functions**

A crucial aspect of using Supabase Vault is understanding its access model. The decrypted secrets are exposed through a view called vault.decrypted_secrets. For security reasons, this view cannot be queried directly by a standard authenticated user role from a client application or an Edge Function.30 Attempting to do so would result in a permission denied error, as it would bypass the security model of the Vault.

The correct and secure pattern is to create a SECURITY DEFINER Postgres function. This type of function executes with the permissions of the user who defined it (the postgres role), not the user who calls it. This allows the function to access privileged resources like vault.decrypted_secrets in a controlled manner. The Edge Function will then call this specific function via rpc(), creating a secure, well-defined boundary and preventing direct access to the secrets view.31

The following SQL creates the necessary functions to securely write and read the refresh token.

SQL

\-- Continues in supabase/migrations/YYYYMMDDHHMMSS_add_user_tokens_table.sql

\-- Function to securely store a new or updated refresh token.  
\-- This function runs with the permissions of the user calling it.  
CREATE OR REPLACE FUNCTION public.store_google_refresh_token(p_user_id UUID, p_refresh_token TEXT)  
RETURNS VOID AS $$  
BEGIN  
 INSERT INTO public.user_tokens (user_id, encrypted_google_refresh_token)  
 VALUES (p_user_id, pgsodium.crypto_aead_encrypt(p_refresh_token::BYTEA, '{}'::BYTEA, (SELECT id FROM vault.secrets WHERE name \= 'google_oauth_key_id' LIMIT 1)))  
 ON CONFLICT (user_id) DO UPDATE  
 SET  
 encrypted_google_refresh_token \= EXCLUDED.encrypted_google_refresh_token,  
 updated_at \= NOW();  
END;

$$
LANGUAGE plpgsql;

\-- SECURITY DEFINER function to securely retrieve and decrypt a refresh token.
\-- This is the only way the backend should access the token.
CREATE OR REPLACE FUNCTION private.get\_decrypted\_google\_refresh\_token(p\_user\_id UUID)
RETURNS TEXT AS
$$

DECLARE  
 decrypted_token TEXT;  
BEGIN  
 \-- This function runs as the user who defined it (postgres),  
 \-- allowing it to access the decrypted secrets view.  
 SELECT pgsodium.crypto_aead_decrypt(t.encrypted_google_refresh_token, '{}'::BYTEA, (SELECT id FROM vault.secrets WHERE name \= 'google_oauth_key_id' LIMIT 1))::TEXT  
 INTO decrypted_token  
 FROM public.user_tokens t  
 WHERE t.user_id \= p_user_id;

    RETURN decrypted\_token;

END;

$$
LANGUAGE plpgsql SECURITY DEFINER;

\-- Note: The above functions assume you have created a key in Vault and named it 'google\_oauth\_key\_id'.
\-- You can create a key using: SELECT pgsodium.create\_key(name := 'google\_oauth\_key\_id');
\-- For simplicity, this example uses pgsodium functions directly, which Vault wraps.
\-- A Vault-centric approach would use vault.create\_secret and query vault.decrypted\_secrets.
\-- Let's stick to the \`user\_tokens\` table pattern with a SECURITY DEFINER function.
\-- The \`private\` schema ensures the function is not exposed via the public API.

*Note: The above SQL uses pgsodium functions for encryption/decryption as Vault is a usability layer on top of it. A key must be created in Vault first. For this guide, we will assume a key has been created and its ID is retrievable.*

#### **Applying Row Level Security (RLS)**

Finally, to complete the defense-in-depth strategy, Row Level Security (RLS) policies are applied to the bookings and user\_tokens tables. RLS ensures that data access is filtered at the database level, preventing accidental data leaks even if there is a bug in the application logic.32

SQL

\-- Continues in supabase/migrations/YYYYMMDDHHMMSS\_add\_user\_tokens\_table.sql

\-- Enable RLS on all relevant tables
ALTER TABLE public.bookings ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.user\_tokens ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.profiles ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.services ENABLE ROW LEVEL SECURITY;

\-- POLICIES FOR 'bookings' TABLE
\-- Policy: Users (clients) can only see their own bookings.
CREATE POLICY "Clients can view their own bookings"
ON public.bookings FOR SELECT
TO authenticated
USING (user\_id \= auth.uid());

\-- Policy: Consultants can only see bookings assigned to them.
CREATE POLICY "Consultants can view their assigned bookings"
ON public.bookings FOR SELECT
TO authenticated
USING (consultant\_id \= auth.uid());

\-- Policy: Authenticated users can create bookings for themselves.
CREATE POLICY "Authenticated users can insert bookings"
ON public.bookings FOR INSERT
TO authenticated
WITH CHECK (user\_id \= auth.uid());

\-- Policy: Users can update their own bookings (e.g., to cancel).
CREATE POLICY "Users can update their own bookings"
ON public.bookings FOR UPDATE
TO authenticated
USING (user\_id \= auth.uid() OR consultant\_id \= auth.uid());

\-- POLICIES FOR 'user\_tokens' TABLE
\-- Policy: A user can only access their own token record.
\-- This policy applies even to the SECURITY DEFINER function call,
\-- as the \`auth.uid()\` is evaluated based on the original caller's JWT.
CREATE POLICY "Users can manage their own tokens"
ON public.user\_tokens FOR ALL
TO authenticated
USING (user\_id \= auth.uid())
WITH CHECK (user\_id \= auth.uid());

\-- POLICIES FOR 'profiles' and 'services'
\-- Allow authenticated users to read profiles and services.
CREATE POLICY "Authenticated users can view profiles"
ON public.profiles FOR SELECT
TO authenticated
USING (true);

CREATE POLICY "Authenticated users can view services"
ON public.services FOR SELECT
TO authenticated
USING (true);

This combination of Vault encryption, SECURITY DEFINER functions, and RLS creates a formidable security posture for managing the highly sensitive Google refresh tokens.

| Feature | Purpose | Implementation Detail |
| :---- | :---- | :---- |
| **OAuth 2.0 Offline Access** | Obtain a long-lived refresh token for server-side, non-interactive API calls. | Set access\_type=offline in the initial authorization request. |
| **Supabase Vault** | Encrypt the refresh token at rest within the PostgreSQL database. | The user\_tokens table stores the token in an encrypted BYTEA column. |
| **SECURITY DEFINER Function** | Provide a secure, controlled bridge for backend services to access decrypted secrets. | A private schema function (get\_decrypted\_google\_refresh\_token) is created to access vault.decrypted\_secrets, callable only via rpc(). |
| **Row Level Security (RLS)** | Enforce that an authenticated user can only request their own token, even via the secure function. | Policies on user\_tokens and bookings tables use auth.uid() to filter rows based on the caller's identity. |

## **Part 2: Core Functionality: Synchronous Meeting Creation via Supabase Edge Functions**

With the secure foundation in place, the next step is to build the core feature of the application: creating a booking and its associated Google Meet link. This part outlines a synchronous approach using Supabase Edge Functions. This method provides immediate feedback to the user upon booking, making it an excellent choice for the initial implementation before evolving to a more complex, asynchronous architecture.

### **2.1 Architecting the Supabase Edge Function**

Supabase Edge Functions are server-side TypeScript functions that run on Deno, a modern and secure runtime.35 They are globally distributed, which means they execute close to the user making the request, minimizing latency. For this system, an Edge Function named

create-google-event will encapsulate the entire logic of interacting with the Google Calendar API.

#### **Local Development and Dependency Management**

Development begins locally using the Supabase CLI. The following command scaffolds a new function:

Bash

supabase functions new create-google-event

This creates a directory supabase/functions/create-google-event with an index.ts file.36

A key advantage of modern Supabase Edge Functions is their native support for npm modules, which bridges the gap between the Deno and Node.js ecosystems.37 This allows the use of official and well-maintained libraries like

googleapis and google-auth-library. These packages can be imported directly within the Deno function using the npm: specifier, eliminating the need for complex build steps.39

TypeScript

// supabase/functions/create-google-event/index.ts

import { createClient } from 'jsr:@supabase/supabase-js@2';
import { google } from 'npm:googleapis@148.0.0';
import { JWT } from 'npm:google-auth-library';
import { corsHeaders } from '../\_shared/cors.ts';

//... function logic...

It is important to note that developers have sometimes encountered issues with npm:googleapis in the Deno environment, often related to underlying dependencies or changes in the Edge Runtime backend.39 Using specific, tested versions (e.g.,

npm:googleapis@148.0.0) and ensuring the Supabase CLI is up-to-date can mitigate these problems.

### **2.2 The Token Refresh Lifecycle in Deno**

Inside the Edge Function, the first task is to obtain a valid access token for the consultant on whose calendar the event will be created. This involves retrieving their stored refresh token and using it to request a new access token from Google.

The process is as follows:

1. **Instantiate Supabase Client:** Create a Supabase client instance within the function. It's crucial to use the service\_role key to bypass RLS for administrative tasks like calling a private schema RPC function and fetching booking details.
2. **Receive Booking ID:** The function will be invoked via a POST request, with the booking\_id in the request body.
3. **Fetch Booking and Service Data:** Use the admin client to fetch the full booking record from the bookings table and the related service details (specifically duration\_minutes) from the services table.
4. **Fetch Refresh Token:** Call the private.get\_decrypted\_google\_refresh\_token database function created in Part 1\. This is done using the rpc() method of the Supabase client, passing the consultant's user\_id from the booking record. This is the only point where the decrypted token is handled.
5. **Generate Access Token:** Initialize a Google OAuth2 client from the google-auth-library with the application's CLIENT\_ID, CLIENT\_SECRET, and the retrieved refresh token. Use this client to get a new, short-lived access token.

Here is the code implementing this token lifecycle:

TypeScript

// supabase/functions/create-google-event/index.ts (continued)

Deno.serve(async (req) \=\> {
  if (req.method \=== 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders });
  }

  try {
    const { booking\_id } \= await req.json();

    // 1\. Create a Supabase client with the service role key to call private RPCs and fetch data
    const supabaseAdmin \= createClient(
      Deno.env.get('SUPABASE\_URL')\!,
      Deno.env.get('SUPABASE\_SERVICE\_ROLE\_KEY')\!
    );

    // 2\. Fetch the booking and related service data
    const { data: bookingData, error: bookingError } \= await supabaseAdmin
     .from('bookings')
     .select(\`
        \*,
        service:services(duration\_minutes)
      \`)
     .eq('id', booking\_id)
     .single();

    if (bookingError ||\!bookingData) {
      throw new Error('Booking not found or failed to fetch.');
    }

    // 3\. Securely fetch the consultant's refresh token
    const { data: refreshToken, error: rpcError } \= await supabaseAdmin.rpc(
      'get\_decrypted\_google\_refresh\_token',
      { p\_user\_id: bookingData.consultant\_id }
    );

    if (rpcError ||\!refreshToken) {
      console.error('Error fetching refresh token:', rpcError);
      throw new Error('Could not retrieve refresh token for consultant.');
    }

    // 4\. Use the refresh token to get a new access token
    const oauth2Client \= new google.auth.OAuth2(
      Deno.env.get('GOOGLE\_CLIENT\_ID')\!,
      Deno.env.get('GOOGLE\_CLIENT\_SECRET')\!
    );

    oauth2Client.setCredentials({ refresh\_token: refreshToken });

    const { token: accessToken } \= await oauth2Client.getAccessToken();

    if (\!accessToken) {
      throw new Error('Failed to generate Google API access token.');
    }

    //... next steps: create calendar event...

  } catch (error) {
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: {...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

### **2.3 Creating Meetings with the Google Calendar API**

With a valid access token, the function can now interact with the Google Calendar API. The goal is to use the events.insert method to create a new event, which will automatically generate the Google Meet link.

#### **Constructing the Event Payload**

The key to this operation is the structure of the event resource payload. It must include not only standard event details but also a specific conferenceData object.

* summary, description: Standard calendar event properties, populated from the booking data (e.g., project\_description).
* start, end: The start.dateTime is taken from bookings.scheduled\_datetime. The end.dateTime is calculated by adding the services.duration\_minutes to the start time.
* attendees: An array of objects, each with an email property. This should include the client's email from bookings.customer\_data and the consultant's email (which would need to be fetched from the auth.users table).
* conferenceData: This object instructs Google to create a video conference.
  * createRequest: Contains the parameters for the new conference.
  * requestId: A unique string for this request to prevent duplicate conference creation on retries. The booking\_id is a perfect candidate.
  * conferenceSolutionKey: Specifies the conference provider. For Google Meet, this is { type: 'hangoutsMeet' }.
* conferenceDataVersion=1: This is a mandatory query parameter for the events.insert call when conferenceData is included in the request body. It signals to the API that conference data should be processed.41

The following code demonstrates the API call:

TypeScript

//... inside the Deno.serve try block, after getting the access token...

// 5\. Initialize the Google Calendar API client
const calendar \= google.calendar({ version: 'v3', auth: oauth2Client });

// 6\. Calculate end time
const startTime \= new Date(bookingData.scheduled\_datetime);
const endTime \= new Date(startTime.getTime() \+ bookingData.service.duration\_minutes \* 60000);

// 7\. Construct the event payload
const event \= {
  summary: \`Booking with ${bookingData.customer\_data.first\_name}\`,
  description: bookingData.project\_description,
  start: {
    dateTime: startTime.toISOString(),
    timeZone: 'UTC', // Assuming all datetimes are stored in UTC
  },
  end: {
    dateTime: endTime.toISOString(),
    timeZone: 'UTC',
  },
  attendees:,
  conferenceData: {
    createRequest: {
      requestId: bookingData.id, // Use booking ID for idempotency
      conferenceSolutionKey: {
        type: 'hangoutsMeet',
      },
    },
  },
};

// 8\. Call the Google Calendar API to insert the event
const { data: createdEvent } \= await calendar.events.insert({
  calendarId: 'primary', // Use the consultant's primary calendar
  requestBody: event,
  conferenceDataVersion: 1, // This is required to generate the Meet link
  sendUpdates: 'all', // Send notifications to attendees
});

if (\!createdEvent) {
  throw new Error('Failed to create Google Calendar event.');
}

//... next step: update the Supabase booking record...

### **2.4 Closing the Loop: Updating the Booking Record**

After the Google Calendar event is successfully created, the API response will contain the htmlLink (a link to the event in the Google Calendar UI) and hangoutLink (the direct Google Meet URL). The final step within the Edge Function is to persist these links back to the bookings table in Supabase.

This is achieved by using the same supabaseAdmin client to perform an update operation, targeting the meeting\_url and meeting\_id columns, filtered by the booking\_id.

TypeScript

//... inside the Deno.serve try block, after creating the event...

// 9\. Update the booking record in Supabase with the new links
const { error: updateError } \= await supabaseAdmin
.from('bookings')
.update({
    meeting\_id: createdEvent.id, // Store the Google Calendar event ID
    meeting\_url: createdEvent.hangoutLink, // Store the Google Meet link
    status: 'confirmed', // Update status to confirmed
    confirmed\_at: new Date().toISOString(),
  })
.eq('id', booking\_id);

if (updateError) {
  // Even if the calendar event was created, we need to handle the case
  // where the database update fails. This is a weakness of the synchronous model.
  console.error('Failed to update booking record:', updateError);
  // A compensation logic could be triggered here.
  throw new Error('Calendar event created, but failed to update booking record.');
}

// 10\. Return a success response
return new Response(
  JSON.stringify({
    message: 'Booking confirmed and calendar event created successfully.',
    meetLink: createdEvent.hangoutLink,
    calendarLink: createdEvent.htmlLink,
  }),
  {
    status: 200,
    headers: {...corsHeaders, 'Content-Type': 'application/json' },
  }
);

This final step highlights a potential weakness in the synchronous model: if the database update fails after the calendar event is created, the system is left in an inconsistent state. This is a primary motivation for evolving to the event-driven architecture discussed in Part 3\.

### **2.5 Frontend Integration: Invoking the Edge Function from Next.js**

The final piece is to trigger this entire process from the Next.js frontend. When a user completes the booking form, the application will invoke the create-google-event Edge Function. To do this securely, the call should be made from a server-side context within Next.js, such as a Server Action or an API Route Handler. This prevents exposing the function invocation logic to the client and leverages the secure, cookie-based session management provided by @supabase/ssr.23

Here is an example of a Next.js Server Action that could be called from a booking form:

TypeScript

// app/bookings/actions.ts
'use server';

import { createServerActionClient } from '@supabase/auth-helpers-nextjs';
import { cookies } from 'next/headers';
import { revalidatePath } from 'next/cache';

export async function createBooking(formData: FormData) {
  const supabase \= createServerActionClient({ cookies });

  // 1\. Extract data from form and user session
  const { data: { user } } \= await supabase.auth.getUser();
  if (\!user) {
    throw new Error('User not authenticated');
  }

  const rawFormData \= {
    consultant\_id: formData.get('consultantId') as string,
    service\_id: formData.get('serviceId') as string,
    scheduled\_datetime: formData.get('scheduledDatetime') as string,
    project\_description: formData.get('projectDescription') as string,
    //... other form data
  };

  // 2\. Create the initial booking record in Supabase
  const { data: newBooking, error: insertError } \= await supabase
  .from('bookings')
  .insert({
      user\_id: user.id,
      consultant\_id: rawFormData.consultant\_id,
      service\_id: rawFormData.service\_id,
      scheduled\_datetime: rawFormData.scheduled\_datetime,
      project\_description: rawFormData.project\_description,
      customer\_data: { email: user.email, first\_name: user.user\_metadata.first\_name, last\_name: user.user\_metadata.last\_name },
      //... other required fields from your 'bookings' table
    })
  .select()
  .single();

  if (insertError) {
    console.error('Failed to create initial booking:', insertError);
    return { error: 'Could not create booking.' };
  }

  // 3\. Invoke the Edge Function to create the Google event
  const { data: functionData, error: functionError } \= await supabase.functions.invoke(
    'create-google-event',
    {
      body: {
        booking\_id: newBooking.id,
      },
    }
  );

  if (functionError) {
    console.error('Edge function invocation failed:', functionError);
    // Here you would need to handle the failure, perhaps by deleting the initial booking record.
    return { error: 'Failed to schedule Google Meet.' };
  }

  revalidatePath('/dashboard/bookings');
  return { success: true, data: functionData };
}

This completes the synchronous flow. The user submits a form, a Server Action creates a pending booking, invokes the Edge Function which handles the entire Google API interaction, and updates the booking record. While functional, the user's browser will wait for this entire chain of events to complete, which introduces latency and a single point of failure.

## **Part 3: The Scalable Evolution: Transitioning to an Event-Driven Architecture (EDA)**

The synchronous architecture detailed in Part 2 provides a functional baseline but possesses inherent limitations in resilience and scalability. A user's request is tightly coupled to the availability and performance of external services like the Google Calendar API. Any slowdown or failure in Google's infrastructure directly impacts the user experience, potentially leading to failed bookings and a brittle system. To build a truly robust, scalable, and extensible platform, the architecture must evolve to an event-driven model.

### **3.1 Architectural Deep Dive: Decoupling for Resilience and Extensibility**

An Event-Driven Architecture (EDA) decouples the components of a system by using asynchronous messages, or "events," as the primary means of communication. In this context, instead of the frontend waiting for the Google Calendar API call to complete, it will perform a single, fast operation: writing the confirmed booking to the Supabase database. This action then publishes a booking-confirmed event to a message bus. Independent, backend services subscribe to this event and perform their tasks—like creating the Google Calendar event—asynchronously, without blocking the user's flow.

This shift provides several profound advantages:

* **Enhanced User Experience:** The user receives immediate confirmation that their booking is successful. The perceived performance of the application improves dramatically, as the user is not forced to wait for slower, third-party API calls to complete.
* **Increased Resilience:** If the Google Calendar API is temporarily unavailable or slow, the booking-confirmed event remains safely in the message queue (Google Cloud Pub/Sub). The system can automatically retry processing the event later, ensuring that no bookings are lost due to transient external failures. The initial booking confirmation is unaffected.
* **Improved Scalability:** The service that receives booking requests can be scaled independently of the services that process them. If there is a sudden influx of bookings, the system can absorb the load by queuing the events, while the backend processors work through the backlog at a sustainable pace.
* **Greater Extensibility:** The EDA makes adding new features trivial. To add functionality like "update a CRM" or "send a welcome email" after a booking is confirmed, a new backend service (e.g., another Google Cloud Function) can simply be deployed to subscribe to the same booking-confirmed event. This requires no changes to the existing booking creation or calendar integration logic, fostering a clean, microservices-style architecture.

The following table provides a clear comparison of the two architectural approaches:

| Aspect | Synchronous (Edge Function) | Asynchronous (Pub/Sub \+ Cloud Function) |
| :---- | :---- | :---- |
| **User Experience** | User waits for the Google API response. High latency or failure directly impacts UX. | Instant confirmation for the user. Processing happens in the background. |
| **Resilience** | A single Google API failure or slowdown causes the entire booking to fail. | Failures are isolated. Pub/Sub automatically retries, preventing data loss. |
| **Scalability** | Tied to the scalability and timeout limits of the Supabase Edge Function runtime. | Can scale the processing (Cloud Functions) independently of the booking intake. |
| **Extensibility** | Adding new post-booking logic requires modifying the core create-google-event function. | New logic can be added by deploying a new subscriber function, without touching existing code. |

### **3.2 Publishing Events from Supabase to Google Cloud Pub/Sub**

The first step in implementing the EDA is to make the Supabase database publish an event whenever a new booking is confirmed. This is achieved by combining a PostgreSQL trigger with the pg\_net extension.

While Supabase offers UI-based Database Webhooks, they are less suitable for production environments that rely on Infrastructure-as-Code principles. Defining the trigger and the pg\_net call within a version-controlled SQL migration file ensures that the infrastructure is repeatable, auditable, and can be deployed automatically across different environments (local, staging, production).43

The implementation involves two parts:

1. **A secure function to publish to Pub/Sub:** This database function will be responsible for constructing and sending the HTTP request to the Google Cloud Pub/Sub API. It needs to be a SECURITY DEFINER function to securely access a Google Cloud Service Account key stored in Supabase Vault. This key grants the database permission to publish messages to a specific Pub/Sub topic.
2. **A trigger on the bookings table:** This trigger will fire after a new row is inserted into the bookings table and will execute the publisher function, passing the new booking's data as the payload.

#### **SQL Implementation**

SQL

\-- This migration assumes a Google Cloud Service Account JSON key
\-- has been stored as a secret in Supabase Vault named 'gcp\_pubsub\_publisher\_key'.
\-- It also assumes the Pub/Sub topic details are stored in Vault.

\-- 1\. Create the SECURITY DEFINER function to publish messages.
\-- This function runs with elevated privileges to access secrets.
CREATE OR REPLACE FUNCTION private.publish\_booking\_confirmed\_event()
RETURNS TRIGGER AS
$$

DECLARE  
 pubsub_project_id TEXT;  
 pubsub_topic_id TEXT;  
 service_account_key JSONB;  
 access_token TEXT;  
 request_body JSONB;  
 request_payload JSONB;  
 request_url TEXT;  
 response JSONB;  
BEGIN  
 \-- Retrieve secrets from Vault  
 SELECT decrypted_secret INTO pubsub_project_id FROM vault.decrypted_secrets WHERE name \= 'gcp_pubsub_project_id';  
 SELECT decrypted_secret INTO pubsub_topic_id FROM vault.decrypted_secrets WHERE name \= 'gcp_pubsub_topic_id';  
 SELECT decrypted_secret::jsonb INTO service_account_key FROM vault.decrypted_secrets WHERE name \= 'gcp_pubsub_publisher_key';

    \-- Obtain a GCP access token using the service account key
    SELECT content::jsonb\-\>\>'access\_token'
    INTO access\_token
    FROM http((
        'POST',
        'https://oauth2.googleapis.com/token',
        ARRAY,
        'grant\_type=urn:ietf:params:oauth:grant-type:jwt-bearer\&assertion=' |

| private.sign_jwt(  
 jsonb_build_object(  
 'aud', 'https://oauth2.googleapis.com/token',  
 'scope', 'https://www.googleapis.com/auth/pubsub',  
 'iss', service_account_key\-\>\>'client_email',  
 'iat', extract(epoch from now())::integer,  
 'exp', extract(epoch from now())::integer \+ 3600  
 ),  
 service_account_key\-\>\>'private_key'  
 )  
 ));

    \-- Construct the Pub/Sub message payload
    \-- The data must be base64 encoded.
    request\_body :\= jsonb\_build\_object(
        'data', encode(row\_to\_json(NEW)::text::bytea, 'base64'),
        'attributes', jsonb\_build\_object('source', 'supabase-booking-system')
    );

    request\_payload :\= jsonb\_build\_object('messages', jsonb\_build\_array(request\_body));
    request\_url :\= format('https://pubsub.googleapis.com/v1/projects/%s/topics/%s:publish', pubsub\_project\_id, pubsub\_topic\_id);

    \-- Make the authenticated POST request to Pub/Sub using pg\_net
    SELECT \* INTO response FROM net.http\_post(
        url :\= request\_url,
        headers :\= jsonb\_build\_object(
            'Content-Type', 'application/json',
            'Authorization', 'Bearer ' |

| access_token  
 ),  
 body :\= request_payload  
 );

    \-- Optionally, log the response or handle errors
    IF (response\-\>\>'status\_code')::int\!= 200 THEN
        RAISE WARNING 'Failed to publish to Pub/Sub: %', response;
    END IF;

    RETURN NEW;

END;

$$
LANGUAGE plpgsql SECURITY DEFINER;

\-- (Helper function private.sign\_jwt would need to be created using pgsodium for JWT signing)

\-- 2\. Create the trigger on the bookings table.
CREATE TRIGGER on\_booking\_insert\_publish\_event
AFTER INSERT ON public.bookings
FOR EACH ROW
EXECUTE FUNCTION private.publish\_booking\_confirmed\_event();

*Note: The above SQL is a conceptual representation. It requires a helper function private.sign\_jwt for creating the JWT assertion from the service account key, which can be implemented using the pgsodium extension.*

Now, every INSERT into the bookings table will asynchronously trigger an authenticated call to Google Cloud Pub/Sub, publishing the new booking data without delaying the original transaction.

### **3.3 Asynchronous Processing with Google Cloud Functions**

The receiving end of the event-driven flow is a Google Cloud Function. This function will be configured to trigger whenever a new message is published to the booking-confirmed Pub/Sub topic.10

#### **Creating a Pub/Sub-Triggered Cloud Function**

Using the gcloud CLI or the Google Cloud Console, a new Cloud Function can be created with the following configuration:

* **Trigger Type:** Google Cloud Pub/Sub.
* **Event Type:** google.cloud.pubsub.topic.v1.messagePublished.10
* **Pub/Sub Topic:** Select the booking-confirmed topic created earlier.
* **Runtime:** Node.js (or any other preferred runtime).

The Cloud Function will receive an event object that contains the message payload. The actual data from the Supabase trigger (the booking record) is in the message.data field, encoded as a Base64 string. The function's first step is to decode this data to get the booking details.11

JavaScript

// index.js for the Google Cloud Function

const functions \= require('@google-cloud/functions-framework');

functions.cloudEvent('processBooking', (cloudEvent) \=\> {
  // The Pub/Sub message is passed as the cloudEvent.data.message property.
  const base64Booking \= cloudEvent.data.message.data;

  const bookingString \= base64Booking
  ? Buffer.from(base64Booking, 'base64').toString()
    : '{}';

  const bookingData \= JSON.parse(bookingString);

  console.log(\`Processing booking ID: ${bookingData.id}\`);

  // Now, port the logic from the old Edge Function here.
  // 1\. Fetch consultant's refresh token from Supabase Vault (via a secure mechanism).
  // 2\. Fetch related service data from Supabase to get duration.
  // 3\. Generate a new Google access token.
  // 4\. Call the Google Calendar API to create the event.
  // 5\. Update the booking record in Supabase with the event links.
});

### **3.4 Porting the Logic and Building for Failure**

The core application logic previously housed in the Supabase Edge Function (Section 2.2 \- 2.4) is now migrated into this new Google Cloud Function. This includes fetching the refresh token, fetching service details, generating an access token, calling the Calendar API, and updating the Supabase database.

A critical advantage of this architecture is the ability to configure robust error handling at the Pub/Sub level. The Pub/Sub subscription for the Cloud Function should be configured with:

* **Retry Policy:** Enable automatic retries with an exponential backoff delay. If the function fails to process a message (e.g., due to a temporary Google Calendar API outage), Pub/Sub will automatically attempt to deliver the message again after a delay, preventing data loss.
* **Dead-Letter Queue (DLQ):** Configure a separate Pub/Sub topic as a DLQ. If a message repeatedly fails processing after all retry attempts, Pub/Sub will move it to the DLQ. This prevents a single "poison pill" message from blocking the entire queue and allows developers to manually inspect and reprocess failed events later.

By transitioning to this event-driven model, the booking system becomes significantly more resilient, scalable, and prepared for future feature enhancements.

## **Part 4: The Future-Proofed Ecosystem: Advanced Integrations and AI Insights**

The event-driven architecture established in Part 3 is not just a solution for resilience; it is a platform for innovation. By decoupling system components, it becomes straightforward to add advanced features by simply creating new, independent services that subscribe to existing events. This section outlines the implementation of the forward-looking features from the project roadmap, demonstrating the true power of the EDA.

### **4.1 Capturing Post-Meeting Artifacts with the Google Workspace Events API**

Once a meeting concludes, valuable artifacts like recordings and transcripts are often generated. Manually retrieving and associating these with the original booking is inefficient. The Google Workspace Events API provides a programmatic way to automate this process.46

#### **Architecture for Artifact Ingestion**

1. **Subscription:** The application will use the Google Workspace Events API to create a subscription to events for a specific meeting space or for all meeting spaces belonging to a user.9 The target event type will be related to conference artifacts, such as
   google.workspace.meet.recording.v2.created.
2. **Notification Endpoint:** This subscription will be configured to use a new Google Cloud Pub/Sub topic (e.g., meet-artifact-ready) as its notification endpoint.46
3. **Processing Function:** A new Google Cloud Function, process-meet-artifact, will be created to subscribe to this meet-artifact-ready topic.

#### **Implementation Steps**

When a meeting recording is processed and saved to the organizer's Google Drive, the Google Workspace Events API will publish a CloudEvent to the specified Pub/Sub topic. The process-meet-artifact function will then execute:

1. **Parse the Event:** The function will parse the incoming CloudEvent to extract details about the artifact, including its resource name or file ID in Google Drive.46
2. **Access Google Drive:** Using the consultant's stored refresh token (retrieved securely from Supabase Vault), the function will obtain a Google Drive API access token.
3. **Retrieve File Link:** It will then use the Google Drive API's files.get method to retrieve metadata for the recording file, specifically its webViewLink or a shareable link.47
4. **Update Supabase:** Finally, the function will connect to the Supabase database and update the bookings table. To support this, you would add a recording\_url column to your bookings table and populate it with the link.

This fully automated workflow ensures that as soon as a recording is available, it is immediately linked to the correct booking record in the application, providing significant value to users.

### **4.2 Intelligent Transcription and Analysis**

A raw video recording is useful, but a searchable text transcript is even more powerful. This can be achieved by chaining another function to the artifact ingestion flow. Your voice\_consultations table already indicates a need for storing transcripts and summaries. We can adapt this concept for our video meetings by adding similar columns directly to the bookings table for simplicity.

#### **Chaining Cloud Functions for Transcription**

The process-meet-artifact function can be modified to not only save the recording link but also to trigger the next step in the pipeline: transcription. After confirming the recording is available, it can publish a new event to another Pub/Sub topic, such as transcription-requested, with the Drive file ID as the payload.

A new Google Cloud Function, transcribe-recording, will subscribe to this topic. Its sole responsibility is to orchestrate the transcription process:

1. **Download Audio:** The function will use the Google Drive API to download the audio content of the meeting recording.47
2. **Call Speech-to-Text API:** The audio data will then be sent to the **Google Cloud Speech-to-Text API**. For long-form audio like meetings, the recognize method with a GCS URI or the streaming recognition API is most appropriate.50
3. **Save Transcript:** The API will return a full text transcript of the meeting. This text is then saved back to a new transcript column in the bookings table in Supabase (you would need to add this column via a migration).

JavaScript

// Conceptual Node.js code for the transcribe-recording Cloud Function
const { SpeechClient } \= require('@google-cloud/speech');
const speechClient \= new SpeechClient();

async function transcribeAudioFromDrive(gcsUri) {
  const audio \= {
    uri: gcsUri, // e.g., 'gs://your-bucket/audio.wav'
  };
  const config \= {
    encoding: 'LINEAR16',
    sampleRateHertz: 16000,
    languageCode: 'en-US',
    enableAutomaticPunctuation: true,
  };
  const request \= {
    audio: audio,
    config: config,
  };

  const \[response\] \= await speechClient.recognize(request);
  const transcription \= response.results
  .map(result \=\> result.alternatives.transcript)
  .join('\\n');

  return transcription;
}

### **4.3 Deriving Value with AI: The Natural Language API**

With a full text transcript available, the system can now apply artificial intelligence to extract structured insights, summaries, and sentiment. This transforms unstructured conversation into valuable, queryable data.

A new database trigger on the bookings table can monitor for updates to the transcript column. When a new transcript is added, the trigger can publish a transcript-ready event to a new Pub/Sub topic.

A final Cloud Function in this pipeline, analyze-transcript, will subscribe to this event.

1. **Retrieve Transcript:** The function will receive the booking ID and retrieve the full transcript text from Supabase.
2. **Call Natural Language API:** It will then send the text to the **Google Cloud Natural Language API**. This API offers a suite of text analysis tools.52
   * analyzeEntities: To identify and extract key entities like people, organizations, and locations mentioned in the meeting.
   * classifyText: To determine the high-level topics of the conversation (e.g., "Project Planning," "Finance," "Technology").
   * analyzeSentiment: To gauge the overall sentiment (positive, negative, neutral) of the discussion.
3. **Store Insights:** The structured output from the Natural Language API (entities, categories, sentiment scores) is then saved back into a new, dedicated JSONB column (e.g., ai\_insights) in the bookings table in Supabase. This makes the insights searchable and displayable within the application's UI.

JavaScript

// Conceptual Node.js code for the analyze-transcript Cloud Function
const language \= require('@google-cloud/language');
const languageClient \= new language.LanguageServiceClient();

async function analyzeText(text) {
  const document \= {
    content: text,
    type: 'PLAIN\_TEXT',
  };

  // Detects the sentiment of the text
  const \= await languageClient.analyzeSentiment({ document });
  const sentiment \= sentimentResult.documentSentiment;

  // Detects entities in the text
  const \= await languageClient.analyzeEntities({ document });
  const entities \= entitiesResult.entities;

  return { sentiment, entities };
}

### **4.4 Proactive Custom Notifications with Cloud Tasks and FCM**

While Google Calendar provides its own reminders, the application can offer a more customized and proactive notification experience using a combination of Google Cloud Tasks and Firebase Cloud Messaging (FCM).

#### **Architecture for Scheduled Notifications**

1. **Task Enqueueing:** When the create-google-event function (from Part 3\) runs, in addition to its other duties, it will also create a new task in **Google Cloud Tasks**. This task will be scheduled to execute at a specific time, for example, 15 minutes before the scheduled\_datetime of the booking.54
2. **HTTP Target:** The Cloud Task will be configured with an HTTP Target, which is the invocation URL of a new, dedicated HTTP-triggered Google Cloud Function (e.g., send-meeting-reminder).
3. **Task Execution:** At the scheduled time, Cloud Tasks will make a POST request to the send-meeting-reminder function's URL, triggering its execution.
4. **Push Notification:** The send-meeting-reminder function will contain the logic to send a notification. It will use the **Firebase Cloud Messaging (FCM)** Admin SDK to send a push notification to the client's and/or consultant's registered devices (mobile or web).55 The payload of the notification can be customized with deep links and relevant meeting information.

This architecture provides full control over the timing, content, and delivery mechanism of reminders, creating a richer user experience that goes beyond the default calendar notifications.

## **Conclusion: A Unified, Scalable Booking System**

This comprehensive guide has detailed the architectural journey of building a modern, production-grade booking system. By strategically integrating Next.js, Supabase, and a suite of Google Cloud services, the resulting platform is not only functional but also secure, resilient, and primed for future expansion.

The journey began with a meticulous focus on the **foundational layer**, establishing a secure Google Cloud project, configuring the necessary APIs with an awareness of cost and quotas, and implementing a robust OAuth 2.0 flow to obtain long-lived refresh tokens for server-side operations. The security of these tokens was paramount, addressed through a multi-layered defense strategy within Supabase, combining the encryption-at-rest capabilities of **Supabase Vault**, the controlled access patterns of **SECURITY DEFINER** functions, and the granular, user-aware filtering of **Row Level Security**.

From this secure base, the **synchronous MVP** was constructed using Supabase Edge Functions. This provided a direct and immediate way to create Google Calendar events with embedded Meet links, offering a tangible and functional core product. However, the analysis of this approach revealed its inherent brittleness and scalability limitations when dealing with external API dependencies.

The architectural evolution to an **event-driven model** using Google Cloud Pub/Sub and Cloud Functions represents the transition from a simple application to a scalable platform. By decoupling the initial booking request from the backend processing, the system gains immense resilience against transient failures, enhances user-perceived performance, and creates an extensible framework. Adding new features no longer requires modifying core logic but simply involves deploying new, independent subscriber functions.

Finally, the report demonstrated the power of this event-driven ecosystem by outlining a clear path to implementing **advanced, future-proof features**. From automatically capturing post-meeting recordings and transcripts with the Google Workspace Events and Drive APIs, to deriving AI-powered insights with Speech-to-Text and the Natural Language API, and delivering proactive custom notifications with Cloud Tasks and FCM, the architecture proves its value as a platform for continuous innovation.

In conclusion, the system described herein achieves all the initial objectives. It is a fast, reliable, and intelligent booking platform that leverages the best of each technology: the modern frontend experience of Next.js, the secure and integrated backend-as-a-service capabilities of Supabase, and the powerful, scalable, and intelligent services of the Google Cloud ecosystem. This is a blueprint for building not just a booking system, but a sophisticated, event-driven platform ready to meet the demands of today and the opportunities of tomorrow.
$$
